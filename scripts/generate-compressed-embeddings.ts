import fs from 'fs/promises';
import path from 'path';
import zlib from 'zlib';
import { pipeline, FeatureExtractionPipeline } from '@xenova/transformers';
import pdf from 'pdf-parse';

// --- CONFIGURACI√ìN OPTIMIZADA ---
const PDFS_DIRECTORY = path.join(__dirname, '../corpus-pdfs');
const OUTPUT_FILE = path.join(__dirname, '../app/embeddings-compressed.json');
const CHUNK_SIZE = 600; // Reducido para mejor compresi√≥n
const CHUNK_OVERLAP = 200;
const MIN_CHUNK_SIZE = 80;

// --- CLASE PARA LA GENERACI√ìN DE EMBEDDINGS ---
class EmbeddingGenerator {
  private static instance: FeatureExtractionPipeline | null = null;

  static async getInstance(): Promise<FeatureExtractionPipeline> {
    if (this.instance === null) {
      console.log('‚è≥ Inicializando modelo de embeddings...');
      this.instance = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2', {
        quantized: true,
      }) as FeatureExtractionPipeline;
      console.log('‚úÖ Modelo de embeddings cargado.');
    }
    return this.instance;
  }
}

// --- FUNCI√ìN DE COMPRESI√ìN ---
async function compressEmbeddings(embeddingsData: any): Promise<Buffer> {
  const jsonString = JSON.stringify(embeddingsData);
  return new Promise((resolve, reject) => {
    zlib.gzip(jsonString, { level: 9 }, (err, result) => {
      if (err) reject(err);
      else resolve(result);
    });
  });
}

// --- FUNCI√ìN DE DESCOMPRESI√ìN ---
async function decompressEmbeddings(compressedData: Buffer): Promise<any> {
  return new Promise((resolve, reject) => {
    zlib.gunzip(compressedData, (err, result) => {
      if (err) reject(err);
      else resolve(JSON.parse(result.toString()));
    });
  });
}

// --- FUNCI√ìN PRINCIPAL ---
async function generateCompressedEmbeddings() {
  try {
    console.log('üöÄ Iniciando generaci√≥n de embeddings comprimidos...');

    // 1. Verificar directorio
    try {
      await fs.access(PDFS_DIRECTORY);
    } catch (e) {
      console.error(`‚ùå Directorio '${PDFS_DIRECTORY}' no existe.`);
      return;
    }

    const files = await fs.readdir(PDFS_DIRECTORY);
    const pdfFiles = files.filter((file) => path.extname(file).toLowerCase() === '.pdf');

    if (pdfFiles.length === 0) {
      console.log('ü§∑ No se encontraron PDFs en corpus-pdfs/');
      return;
    }

    console.log(`üìÑ Procesando ${pdfFiles.length} archivos PDF...`);

    const allEmbeddings = [];
    const embedder = await EmbeddingGenerator.getInstance();

    for (const pdfFile of pdfFiles) {
      console.log(`\n--- üîÑ Procesando: ${pdfFile} ---`);
      const filePath = path.join(PDFS_DIRECTORY, pdfFile);
      const fileBuffer = await fs.readFile(filePath);

      try {
        // Parse completo del PDF
        const pdfData = await pdf(fileBuffer, {
          max: 0,
          version: 'v1.10.100'
        });

        if (!pdfData.text || pdfData.text.trim().length === 0) {
          console.log(`  ‚ö†Ô∏è ${pdfFile}: Sin texto extra√≠ble.`);
          continue;
        }

        console.log(`  üìÑ Extra√≠do: ${pdfData.text.length} caracteres`);

        // Limpiar texto
        const cleanedText = cleanText(pdfData.text);

        // Crear chunks m√°s peque√±os para mejor compresi√≥n
        const chunks = smartChunking(cleanedText, CHUNK_SIZE, CHUNK_OVERLAP);
        console.log(`  üî™ Dividido en ${chunks.length} chunks`);

        let processedChunks = 0;
        for (const chunk of chunks) {
          try {
            const output = await embedder(chunk.content, {
              pooling: 'mean',
              normalize: true
            });
            
            const embedding = Array.from(output.data as Float32Array);

            allEmbeddings.push({
              content: chunk.content,
              embedding: embedding,
              metadata: {
                fileName: pdfFile.replace('.pdf', ''),
                chunkIndex: processedChunks,
                chunkStart: chunk.startIndex,
                chunkEnd: chunk.endIndex,
                contentType: 'transcript',
                chunkSize: chunk.content.length
              },
            });

            processedChunks++;

            if (processedChunks % 10 === 0) {
              console.log(`    ‚ö° Procesados ${processedChunks}/${chunks.length} chunks`);
            }

          } catch (embeddingError: unknown) {
            const errorMessage = embeddingError instanceof Error ? embeddingError.message : String(embeddingError);
            console.warn(`    ‚ö†Ô∏è Error en chunk ${processedChunks}:`, errorMessage);
            continue;
          }
        }

        console.log(`  ‚úÖ ${pdfFile}: ${processedChunks} chunks procesados`);

      } catch (pdfError: unknown) {
        const errorMessage = pdfError instanceof Error ? pdfError.message : String(pdfError);
        console.error(`  ‚ùå Error procesando ${pdfFile}:`, errorMessage);
        continue;
      }
    }

    // 4. Crear estructura final
    const outputData = {
      metadata: {
        generatedAt: new Date().toISOString(),
        model: 'Xenova/all-MiniLM-L6-v2',
        chunkSize: CHUNK_SIZE,
        chunkOverlap: CHUNK_OVERLAP,
        totalDocuments: pdfFiles.length,
        totalEmbeddings: allEmbeddings.length,
        embeddingDimensions: 384,
        compression: 'gzip-level9'
      },
      embeddings: allEmbeddings
    };

    console.log(`\nüíæ Generando archivo comprimido...`);

    // 5. Comprimir los datos
    const compressedBuffer = await compressEmbeddings(outputData);

    // 6. Guardar archivo comprimido
    await fs.writeFile(OUTPUT_FILE + '.gz', compressedBuffer);
    
    // 7. Tambi√©n guardar versi√≥n sin comprimir para desarrollo
    await fs.writeFile(OUTPUT_FILE, JSON.stringify(outputData, null, 2));

    // 8. Mostrar estad√≠sticas
    const originalSize = JSON.stringify(outputData).length;
    const compressedSize = compressedBuffer.length;
    const compressionRatio = ((originalSize - compressedSize) / originalSize * 100).toFixed(1);

    console.log(`üéâ ¬°Completado!
üìä Estad√≠sticas:
   - Documentos: ${pdfFiles.length}
   - Embeddings: ${allEmbeddings.length}
   - Tama√±o original: ${(originalSize / 1024 / 1024).toFixed(1)}MB
   - Tama√±o comprimido: ${(compressedSize / 1024 / 1024).toFixed(1)}MB
   - Compresi√≥n: ${compressionRatio}%
   - Archivo: ${OUTPUT_FILE}.gz`);

  } catch (error: unknown) {
    const errorMessage = error instanceof Error ? error.message : String(error);
    console.error('üí• Error catastr√≥fico:', errorMessage);
  }
}

// --- UTILIDADES DE TEXTO ---
function cleanText(text: string): string {
  return text
    .replace(/\s+/g, ' ')
    .replace(/\n{3,}/g, '\n\n')
    .replace(/[^\x00-\x7F]/g, '')
    .trim();
}

function smartChunking(text: string, maxSize: number, overlap: number): Array<{ content: string, startIndex: number, endIndex: number }> {
  const chunks = [];
  let start = 0;
  
  while (start < text.length) {
    let end = Math.min(start + maxSize, text.length);
    
    if (end < text.length) {
      const lastSentenceEnd = Math.max(
        text.lastIndexOf('.', end),
        text.lastIndexOf('!', end),
        text.lastIndexOf('?', end),
        text.lastIndexOf('\n\n', end)
      );
      
      if (lastSentenceEnd > start + maxSize * 0.5) {
        end = lastSentenceEnd + 1;
      }
    }
    
    const chunk = text.slice(start, end).trim();
    
    if (chunk.length >= MIN_CHUNK_SIZE) {
      chunks.push({
        content: chunk,
        startIndex: start,
        endIndex: end
      });
    }
    
    start = Math.max(start + maxSize - overlap, end);
  }
  
  return chunks;
}

// Ejecutar
generateCompressedEmbeddings();
